{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "79c843de-21e0-4dcc-aec2-fb8a133da849",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ipywidgets import widgets\n",
    "from sklearn import preprocessing\n",
    "import sklearn\n",
    "from datetime import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import plotly.graph_objects as go\n",
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "df8f01af-ce89-43ed-802d-40e6a9175949",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_weighted_average_interpolation = True\n",
    "use_new_features = True\n",
    "nDrives = 150\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dc38425d-cbd7-43bf-bda8-83bbd712f775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if use_weighted_average_interpolation:\n",
    "#     drivePaths = [str(path) for path in Path(\".\").rglob(\"outputs/weightedInterpolation/dataByLocation*.csv\")][:nDrives]\n",
    "# else:\n",
    "#     drivePaths = [str(path) for path in Path(\".\").rglob(\"outputs/unweightedInterpolation/dataByLocation*.csv\")][:nDrives]\n",
    "    \n",
    "# if use_new_features:\n",
    "drivePaths = random.sample([str(path) for path in Path(\".\").rglob(\"outputs/withNewFeatures/dataByLocation*.csv\")], nDrives)\n",
    "drivesUsed = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e7854164-6f51-47da-bcdd-00c03945aaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 128 drives that meet specifications\n"
     ]
    }
   ],
   "source": [
    "subsamplingPeriod = 1\n",
    "\n",
    "drivesWithLocation = []\n",
    "drivesWithoutLocation = []\n",
    "driveIDs = []\n",
    "for drivePath in drivePaths:\n",
    "    drive = pd.read_csv(drivePath)\n",
    "    if len(drive) > 1200 and \"dataByLocation_2021-06-03-19-38-35_2T3Y1RFV8KC014025.csv\" not in drivePath:\n",
    "        driveIDs.append(\"_\".join(drivePath.split(\"/\")[-1].split(\"_\")[1:-1]))\n",
    "        drivesUsed.append(drivePath)\n",
    "        drive = drive.iloc[::subsamplingPeriod]\n",
    "        driveWithoutLocation = drive.drop(columns=[\"Time\",\"Longitude\", \"Latitude\"])\n",
    "#         driveWithoutLocation = driveWithoutLocation.drop(columns=[\"ZAcceleration\", \"LongAcceleration\", \"LatAcceleration\"])\n",
    "        drivesWithLocation.append(drive)\n",
    "        drivesWithoutLocation.append(driveWithoutLocation)\n",
    "print(\"Found\", len(drivesWithoutLocation), \"drives that meet specifications\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99176441-8661-4a80-a9d9-569c117d8040",
   "metadata": {},
   "source": [
    "## Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18eb0077-dd66-4a6b-beb2-8848a6807855",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizedDrives = []\n",
    "combinedDrivesForScalerFitting = pd.concat(drivesWithoutLocation).reset_index().drop(columns=[\"index\"])\n",
    "standard_scaler = preprocessing.StandardScaler()\n",
    "standard_scaler.fit(combinedDrivesForScalerFitting)\n",
    "\n",
    "for drive in drivesWithoutLocation:\n",
    "    drive = drive.values[:]\n",
    "    data_normalized = standard_scaler.transform(drive)\n",
    "    data_normalized = pd.DataFrame(data_normalized)\n",
    "    data_normalized[0] = drive[:,0]\n",
    "    normalizedDrives.append(data_normalized)\n",
    "    \n",
    "scalerFile = open('otherLargeFiles/scalerUsedForTrainingInputs.pkl', 'ab')\n",
    "pickle.dump(standard_scaler, scalerFile)                     \n",
    "scalerFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502bdd98-d432-486c-8697-c70def7a9905",
   "metadata": {},
   "source": [
    "## Generate Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b34b0ad0-95cd-40a0-8204-2b4913f95f23",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on drive 0\n",
      "on drive 1\n",
      "on drive 2\n",
      "on drive 3\n",
      "on drive 4\n",
      "on drive 5\n",
      "on drive 6\n",
      "on drive 7\n",
      "on drive 8\n",
      "on drive 9\n",
      "on drive 10\n",
      "on drive 11\n",
      "on drive 12\n",
      "on drive 13\n",
      "on drive 14\n",
      "on drive 15\n",
      "on drive 16\n",
      "on drive 17\n",
      "on drive 18\n",
      "on drive 19\n",
      "on drive 20\n",
      "on drive 21\n",
      "on drive 22\n",
      "on drive 23\n",
      "on drive 24\n",
      "on drive 25\n",
      "on drive 26\n",
      "on drive 27\n",
      "on drive 28\n",
      "on drive 29\n",
      "on drive 30\n",
      "on drive 31\n",
      "on drive 32\n",
      "on drive 33\n",
      "on drive 34\n",
      "on drive 35\n",
      "on drive 36\n",
      "on drive 37\n",
      "on drive 38\n",
      "on drive 39\n",
      "on drive 40\n",
      "on drive 41\n",
      "on drive 42\n",
      "on drive 43\n",
      "on drive 44\n",
      "on drive 45\n",
      "on drive 46\n",
      "on drive 47\n",
      "on drive 48\n",
      "on drive 49\n",
      "on drive 50\n",
      "on drive 51\n",
      "on drive 52\n",
      "on drive 53\n",
      "on drive 54\n",
      "on drive 55\n",
      "on drive 56\n",
      "on drive 57\n",
      "on drive 58\n",
      "on drive 59\n",
      "on drive 60\n",
      "on drive 61\n",
      "on drive 62\n",
      "on drive 63\n",
      "on drive 64\n",
      "on drive 65\n",
      "on drive 66\n",
      "on drive 67\n",
      "on drive 68\n",
      "on drive 69\n",
      "on drive 70\n",
      "on drive 71\n",
      "on drive 72\n",
      "on drive 73\n",
      "on drive 74\n",
      "on drive 75\n",
      "on drive 76\n",
      "on drive 77\n",
      "on drive 78\n",
      "on drive 79\n",
      "on drive 80\n",
      "on drive 81\n",
      "on drive 82\n",
      "on drive 83\n",
      "on drive 84\n",
      "on drive 85\n",
      "on drive 86\n",
      "on drive 87\n",
      "on drive 88\n",
      "on drive 89\n",
      "on drive 90\n",
      "on drive 91\n",
      "on drive 92\n",
      "on drive 93\n",
      "on drive 94\n",
      "on drive 95\n",
      "on drive 96\n",
      "on drive 97\n",
      "on drive 98\n",
      "on drive 99\n",
      "on drive 100\n",
      "on drive 101\n",
      "on drive 102\n",
      "on drive 103\n",
      "on drive 104\n",
      "on drive 105\n",
      "on drive 106\n",
      "on drive 107\n",
      "on drive 108\n",
      "on drive 109\n",
      "on drive 110\n",
      "on drive 111\n",
      "on drive 112\n",
      "on drive 113\n",
      "on drive 114\n",
      "on drive 115\n",
      "on drive 116\n",
      "on drive 117\n",
      "on drive 118\n",
      "on drive 119\n",
      "on drive 120\n",
      "on drive 121\n",
      "on drive 122\n",
      "on drive 123\n",
      "on drive 124\n",
      "on drive 125\n",
      "on drive 126\n",
      "on drive 127\n"
     ]
    }
   ],
   "source": [
    "# Faster\n",
    "sequenceLength = 10\n",
    "datasetLength = sum([len(drive) for drive in normalizedDrives]) - ((sequenceLength + 1)*len(normalizedDrives))\n",
    "dataset = {\"samples\":np.full((datasetLength, sequenceLength, len(normalizedDrives[0].columns)), -1.),\"labels\":np.full((datasetLength, 2), -1.)}\n",
    "datasetIndex = 0\n",
    "\n",
    "for k,drive in enumerate(normalizedDrives):\n",
    "    print(\"on drive\",k)\n",
    "    for i,sample in drive.iterrows():\n",
    "        if i < len(drive) - sequenceLength - 1:\n",
    "            thisDriveWithLocation = drivesWithLocation[k]\n",
    "            thisSampleWithLocation = thisDriveWithLocation.iloc[i]\n",
    "            thisLong = thisSampleWithLocation.Longitude\n",
    "            thisLat = thisSampleWithLocation.Latitude\n",
    "            nextSample = thisDriveWithLocation.iloc[i+sequenceLength]\n",
    "            nextLong = nextSample.Longitude\n",
    "            nextLat = nextSample.Latitude\n",
    "            deltaLong = nextLong - thisLong\n",
    "            deltaLat = nextLat - thisLat\n",
    "            deltas = [deltaLong, deltaLat]\n",
    "            dataset[\"samples\"][datasetIndex] = [drive.iloc[i+n].to_numpy() for n in range(sequenceLength)]\n",
    "            dataset[\"labels\"][datasetIndex] = deltas\n",
    "            datasetIndex+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5950f51-742c-4a0b-98e3-2d545de3b22d",
   "metadata": {},
   "source": [
    "### Normalize labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbb63034-81fd-41cf-85bb-e223ab52e6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "originalLabels = dataset[\"labels\"]\n",
    "# originalLabels = np.array(originalLabels)\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "labels_normalized = scaler.fit_transform(originalLabels)\n",
    "dataset[\"labels\"] = labels_normalized\n",
    "\n",
    "scalerFile = open('otherLargeFiles/scalerUsedForTrainingLabels.pkl', 'ab')\n",
    "pickle.dump(scaler, scalerFile)                     \n",
    "scalerFile.close()\n",
    "\n",
    "# type(labels_normalized)\n",
    "# labels_normalized = originalLabels\n",
    "# print(labels_normalized.shape)\n",
    "# print(type(labels_normalized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36c0c3c-5232-408a-b708-9dc50b4a78bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1d0a529-e80e-477b-935f-7718f1cf735c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Its important to use binary mode\n",
    "dbfile = open('otherLargeFiles/CNN-dataset.pkl', 'ab')\n",
    "\n",
    "# source, destination\n",
    "pickle.dump(dataset, dbfile)                     \n",
    "dbfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6c519ac9-fbf3-4add-b9e8-299bc9064bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Its important to use binary mode\n",
    "dbfile = open('otherLargeFiles/drives-used.pkl', 'ab')\n",
    "\n",
    "# source, destination\n",
    "pickle.dump(drivesUsed, dbfile)                     \n",
    "dbfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3063b5df-795e-45ea-84c1-2d8082ed8782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "print(len(drivesUsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db3f65c-2674-41e6-b581-ee684a188741",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
