{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb22349-8301-4ddd-8029-e0408ce53e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from ipywidgets import widgets\n",
    "from sklearn import preprocessing\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras import layers, regularizers,Model, utils\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from datetime import time\n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import train_test_split\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from pathlib import Path\n",
    "from livelossplot import PlotLossesKeras\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b95384-e38a-4593-98b9-7645c5fae293",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "drivePaths = [\"outputs/dataByLocation_2020-11-21-16-00-26_5FNYF6H05HB089022.csv\"]\n",
    "drivePaths = [str(path) for path in Path(\".\").rglob(\"outputs/weightedInterpolation/dataByLocation*.csv\")][:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf2d374-8b73-4803-8b12-4423bf3ed255",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "subsamplingPeriod = 1\n",
    "\n",
    "drivesWithLocation = []\n",
    "drivesWithoutLocation = []\n",
    "for drivePath in drivePaths:\n",
    "    drive = pd.read_csv(drivePath)\n",
    "    drive = drive.iloc[::subsamplingPeriod]\n",
    "    driveWithoutLocation = drive.drop(columns=[\"Time\", \"Longitude\", \"Latitude\"])\n",
    "    drivesWithLocation.append(drive)\n",
    "    drivesWithoutLocation.append(driveWithoutLocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd93df7-465f-4969-8cca-03d54fb0def2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "## Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d319d2b-ca5f-45f5-bb9a-3e5cff8b745b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "normalizedDrives = []\n",
    "for drive in drivesWithoutLocation:\n",
    "    drive = drive.values[:]\n",
    "    standard_scaler = preprocessing.StandardScaler()\n",
    "    data_normalized = standard_scaler.fit_transform(drive)\n",
    "    data_normalized = pd.DataFrame(data_normalized)\n",
    "    normalizedDrives.append(data_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5df908d-72e8-4507-b130-b2ec3d4cd9a7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "## Window Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ea798a-56ff-481b-a90d-30a793d7f5c4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sequenceLength = 50\n",
    "features = ['Speed',\n",
    "            'LatAcceleration',\n",
    "            'LongAcceleration',\n",
    "            'SteerTorque',\n",
    "            'SteerRate',\n",
    "            'SteerAngle',\n",
    "            'FLWheelSpeed',\n",
    "            'FRWheelSpeed',\n",
    "            'RRWheelSpeed',\n",
    "            'RLWheelSpeed']\n",
    "\n",
    "windowedDrives = []\n",
    "for drive in normalizedDrives:\n",
    "    data_df = drive\n",
    "    stackedData = []\n",
    "    # split can_data into subsampled sequences\n",
    "    for i in range(len(data_df)-sequenceLength):\n",
    "        stackedData.append(data_df[i:i+sequenceLength])\n",
    "    stackedData = np.array(stackedData)\n",
    "    windowedDrives.append(stackedData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdb3ebd-8bd2-444c-b11c-8fdfcb6840a2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "## Generate Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d591e8b3-1cbc-4837-b3ee-4d0175fed431",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = {\"samples\":[], \"labels\":[]}\n",
    "for k,drive in enumerate(windowedDrives):\n",
    "    for i,window in enumerate(drive[:-1]):\n",
    "        last = drivesWithLocation[k].iloc[i]\n",
    "        lastLong = last.Longitude\n",
    "        lastLat = last.Latitude\n",
    "        cur = drivesWithLocation[k].iloc[i+5]\n",
    "        curLong = cur.Longitude\n",
    "        curLat = cur.Latitude\n",
    "        \n",
    "        dataset[\"samples\"].append(window)\n",
    "        dataset[\"labels\"].append([curLong - lastLong, curLat - lastLat])\n",
    "dataset = pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61849443-9587-4545-8a4d-ddda7ef052ab",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Normalize labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a9e7a7-f589-4c7d-b8b5-6ac43f07a7fe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "originalLabels = dataset[\"labels\"].tolist()\n",
    "# originalLabels = np.array(originalLabels)\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "labels_normalized = scaler.fit_transform(originalLabels)\n",
    "\n",
    "dataset[\"labels\"] = labels_normalized\n",
    "labels_normalized.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cae1246-b0ba-4f91-b14b-da7b66b96c5c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Formalize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63dd48d-b632-4f5e-9bad-813f6b5adc2f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "samples = np.stack(dataset[\"samples\"])\n",
    "labels = labels_normalized\n",
    "dataset = {\"samples\": samples, \"labels\": labels}\n",
    "\n",
    "longitudeLabels = dataset[\"labels\"][:,0]\n",
    "latitudeLabels = dataset[\"labels\"][:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7ce396-6de3-423b-b4e9-9dc7d4e5c64f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Build Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf8c366-22d0-496e-addf-4a7ec1dd30ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "datasetFile = open(\"otherLargeFiles/CNN-dataset.pkl\",\"rb\")\n",
    "dataset = pickle.load(datasetFile)\n",
    "datasetFile.close()\n",
    "longitudeLabels = dataset[\"labels\"][:,0]\n",
    "latitudeLabels = dataset[\"labels\"][:,1]\n",
    "samples = dataset[\"samples\"]\n",
    "sequenceLength = samples[0].shape[0]\n",
    "nFeatures = samples[0].shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef6067e-065a-489a-bccf-41b93b548e2c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3baf5c-797c-458b-ad62-46a99d8e48b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Split into training/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538bfa1a-82d4-4cd1-a68a-795f01b74f91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "longTrainInputs, longValInputs, longTrainLabels, longValLabels = train_test_split(dataset[\"samples\"], longitudeLabels, test_size=0.5, shuffle=True)\n",
    "latTrainInputs, latValInputs, latTrainLabels, latValLabels = train_test_split(dataset[\"samples\"], latitudeLabels, test_size=0.5, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647accc0-4e20-4e4b-bdd9-a179907444f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Construct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91e27ab-8c06-4d74-a470-433376306098",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#specify input dimensionality\n",
    "inputDimension = samples[0].shape[0]*samples[0].shape[1]\n",
    "outputDimension = 1\n",
    "\n",
    "#construct our neural network\n",
    "hiddenLayerSizes = [32, 64, 128]\n",
    "\n",
    "#set up our input layer\n",
    "inputLayer = layers.Input(shape=(sequenceLength, nFeatures))\n",
    "\n",
    "#set up our hidden layers\n",
    "curLayer = 0\n",
    "previousLayer = inputLayer\n",
    "for i,curLayerSize in enumerate(hiddenLayerSizes):\n",
    "    previousLayer = layers.Dense(curLayerSize,\n",
    "                                 activation='relu', \n",
    "                                 name=str(curLayerSize)+'_hiddenLayer'+str(i)\n",
    "                                )(previousLayer)\n",
    "    previousLayer = layers.Dropout(\n",
    "                                    rate=0.3\n",
    "                                    )(previousLayer)\n",
    "previousLayer = layers.Flatten()(previousLayer)\n",
    "    \n",
    "outputLayer = layers.Dense(outputDimension, activation='linear')(previousLayer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677e2808-3757-4a16-a0ba-ec3f33d96d08",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdbb2ae-5a3d-4a42-8384-f1962aad14a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#compile separate models\n",
    "longMLP = Model(inputs=inputLayer, outputs=[outputLayer], name='longitude_mlp')\n",
    "latMLP = Model(inputs=inputLayer, outputs=[outputLayer], name='latitude_mlp')\n",
    "longMLP.compile(loss='mean_squared_error', metrics=[tf.keras.metrics.RootMeanSquaredError()], optimizer='adam')\n",
    "latMLP.compile(loss='mean_squared_error', metrics=[tf.keras.metrics.RootMeanSquaredError()], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8becbb87-448a-449d-8b75-39bed21b31ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "longMLP.save(\"models/twin-mlp.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd102a65-6aef-49a4-88f3-807d057f65eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667064a9-db30-4f77-b340-002a2f87822a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "longHistory = longMLP.fit(longTrainInputs, longTrainLabels, epochs=25, batch_size=64, callbacks=[PlotLossesKeras()], validation_data=(longValInputs, longValLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae8b338-df07-41db-8ca1-432b85463c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "latHistory = latMLP.fit(latTrainInputs, latTrainLabels, epochs=25, batch_size=64, callbacks=[PlotLossesKeras()], validation_data=(latValInputs, latValLabels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90844967-6b50-4a50-b177-9751838bce31",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19eb1441-a654-4add-bfaa-7db77d66fe8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Predict Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abab4af1-c74e-4255-9356-1a30d2c1ce23",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "longPredictions = longModel.predict(longValInputs)\n",
    "latPredictions = latModel.predict(latValInputs)\n",
    "\n",
    "scatterData = pd.DataFrame({\n",
    "    \"index\":np.arange(longPredictions.shape[0]),\n",
    "    \"long_predicted\": longPredictions.reshape(longPredictions.shape[0]),\n",
    "    \"long_actual\": longValLabels.reshape(longValLabels.shape[0]),\n",
    "    \"lat_predicted\": latPredictions.reshape(latPredictions.shape[0]),\n",
    "    \"lat_actual\": latValLabels.reshape(latValLabels.shape[0])\n",
    "})\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=scatterData.index, y=scatterData.long_predicted, name=\"long_predicted\"))\n",
    "fig.add_trace(go.Scatter(x=scatterData.index, y=scatterData.long_actual, name=\"long_actual\"))\n",
    "fig.add_trace(go.Scatter(x=scatterData.index, y=scatterData.lat_predicted, name=\"lat_predicted\"))\n",
    "fig.add_trace(go.Scatter(x=scatterData.index, y=scatterData.lat_actual, name=\"lat_actual\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafe3be8-69d1-4c18-b21f-0c8331bbd631",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Predict Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d11f56-6138-48e6-97b5-f67472f243db",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "longPredictions = longModel.predict(longTrainInputs)\n",
    "latPredictions = latModel.predict(latTrainInputs)\n",
    "\n",
    "scatterData = pd.DataFrame({\n",
    "    \"index\":np.arange(longPredictions.shape[0]),\n",
    "    \"long_predicted\": longPredictions.reshape(longPredictions.shape[0]),\n",
    "    \"long_actual\": longTrainLabels.reshape(longTrainLabels.shape[0]),\n",
    "    \"lat_predicted\": latPredictions.reshape(latPredictions.shape[0]),\n",
    "    \"lat_actual\": latTrainLabels.reshape(latTrainLabels.shape[0])\n",
    "})\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=scatterData.index, y=scatterData.long_predicted, name=\"long_predicted\"))\n",
    "fig.add_trace(go.Scatter(x=scatterData.index, y=scatterData.long_actual, name=\"long_actual\"))\n",
    "fig.add_trace(go.Scatter(x=scatterData.index, y=scatterData.lat_predicted, name=\"lat_predicted\"))\n",
    "fig.add_trace(go.Scatter(x=scatterData.index, y=scatterData.lat_actual, name=\"lat_actual\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecc3188-b110-40ab-be20-349ce71d91cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### MLP (combined lat + long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6307d29d-7fa8-4220-9278-4eade3ea5aa9",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Split into training/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264b558f-d4f0-424c-9d3e-f48784137aa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combinedLabels = np.concatenate((np.reshape(longitudeLabels, (-1,1)), np.reshape(latitudeLabels, (-1,1))), axis=1)\n",
    "trainInputs, valInputs, trainLabels, valLabels = train_test_split(dataset[\"samples\"], combinedLabels, test_size=0.5, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4be72c-77cd-4591-83b9-a6be81eeca0e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Construct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfa4b05-8762-4f20-8fe4-6b6b792946c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#specify input dimensionality\n",
    "inputDimension = samples[0].shape[0]*samples[0].shape[1]\n",
    "outputDimension = 2\n",
    "\n",
    "#construct our neural network\n",
    "hiddenLayerSizes = [32, 64, 128]\n",
    "\n",
    "#set up our input layer\n",
    "inputLayer = layers.Input(shape=(sequenceLength, nFeatures))\n",
    "\n",
    "#set up our hidden layers\n",
    "curLayer = 0\n",
    "previousLayer = inputLayer\n",
    "for i,curLayerSize in enumerate(hiddenLayerSizes):\n",
    "    previousLayer = layers.Dense(curLayerSize,\n",
    "                                 activation='relu', \n",
    "                                 name=str(curLayerSize)+'_hiddenLayer'+str(i)\n",
    "                                )(previousLayer)\n",
    "#     previousLayer = layers.LeakyReLU(alpha=0.3)(previousLayer)\n",
    "    previousLayer = layers.Dropout(\n",
    "                                    rate=0.3\n",
    "                                    )(previousLayer)\n",
    "previousLayer = layers.Flatten()(previousLayer)\n",
    "    \n",
    "outputLayer = layers.Dense(outputDimension, activation='linear')(previousLayer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b46c1f9-ad61-463c-87fc-3a25fbdb5d4e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80389d30-90d8-482b-afee-85a0fbaeabde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#compile separate models\n",
    "combinedMLP = Model(inputs=inputLayer, outputs=[outputLayer], name='combined_mlp')\n",
    "combinedMLP.compile(loss='mean_squared_error', metrics=[tf.keras.metrics.RootMeanSquaredError()], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c6a58b-a177-4427-bc5a-cc78a7f866d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedMLP.save(\"models/combined-mlp.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0771c35-48a6-4910-827a-18a0b2bb63bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ab0eb9-a60e-4267-ad9e-ec8a4e0de258",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#train the mlp\n",
    "combinedHistory = combinedMLP.fit(trainInputs, trainLabels, epochs=25, batch_size=32, callbacks=[PlotLossesKeras()], validation_data=(valInputs, valLabels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc1fd3c-476b-4dfc-84a4-6862d16ce996",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb2d553-c430-4098-9862-af8502dd8668",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Predict Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593393ea-d2d4-41ab-8f1d-20c3284a6875",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = combinedModel.predict(valInputs)\n",
    "\n",
    "scatterData = pd.DataFrame({\n",
    "    \"index\":np.arange(predictions[:,0].shape[0]),\n",
    "    \"long_predicted\": predictions[:,0].reshape(predictions[:,0].shape[0]),\n",
    "    \"long_actual\": valLabels[:,0].reshape(valLabels[:,0].shape[0]),\n",
    "    \"lat_predicted\": predictions[:,1].reshape(predictions[:,1].shape[0]),\n",
    "    \"lat_actual\": valLabels[:,1].reshape(valLabels[:,1].shape[0])\n",
    "})\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=scatterData.index, y=scatterData.long_predicted, name=\"long_predicted\"))\n",
    "fig.add_trace(go.Scatter(x=scatterData.index, y=scatterData.long_actual, name=\"long_actual\"))\n",
    "fig.add_trace(go.Scatter(x=scatterData.index, y=scatterData.lat_predicted, name=\"lat_predicted\"))\n",
    "fig.add_trace(go.Scatter(x=scatterData.index, y=scatterData.lat_actual, name=\"lat_actual\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234d9e7d-bdd3-4959-bee4-cae25ace46b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Predict Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2906e8-e2bb-4c5f-a583-627eaaed837e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = combinedModel.predict(trainInputs)\n",
    "\n",
    "scatterData = pd.DataFrame({\n",
    "    \"index\":np.arange(predictions[:,0].shape[0]),\n",
    "    \"long_predicted\": predictions[:,0].reshape(predictions[:,0].shape[0]),\n",
    "    \"long_actual\": trainLabels[:,0].reshape(trainLabels[:,0].shape[0]),\n",
    "    \"lat_predicted\": predictions[:,1].reshape(predictions[:,1].shape[0]),\n",
    "    \"lat_actual\": trainLabels[:,1].reshape(trainLabels[:,1].shape[0])\n",
    "})\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=scatterData.index, y=scatterData.long_predicted, name=\"long_predicted\"))\n",
    "fig.add_trace(go.Scatter(x=scatterData.index, y=scatterData.long_actual, name=\"long_actual\"))\n",
    "fig.add_trace(go.Scatter(x=scatterData.index, y=scatterData.lat_predicted, name=\"lat_predicted\"))\n",
    "fig.add_trace(go.Scatter(x=scatterData.index, y=scatterData.lat_actual, name=\"lat_actual\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac22938-94d9-4b9c-8ed0-8e6407dd2633",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74da5ebd-86c0-4d0f-a650-f3ba37fc639f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainingData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f008e1-2efd-44c8-b30f-dbed103bd599",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#specify input dimensionality\n",
    "numberOfSamples = trainingData.shape[0]\n",
    "inputDimension = trainingData[0].shape[0]*trainingData[0].shape[1]\n",
    "outputDimension = 1\n",
    "\n",
    "#construct our neural network\n",
    "hiddenLayerSizes = [inputDimension, 32, 16, 8, 4, 2]\n",
    "\n",
    "#set up our input layer\n",
    "inputLayer = layers.Input(shape=(inputDimension, 1))\n",
    "\n",
    "#set up our hidden layers\n",
    "curLayer = 0\n",
    "previousLayer = inputLayer\n",
    "for curLayerSize in hiddenLayerSizes:\n",
    "    previousLayer = layers.Conv1D(curLayerSize, 5,\n",
    "                                  activation='sigmoid',\n",
    "                                  name=str(curLayerSize)+'_hiddenLayer',\n",
    "                                  kernel_regularizer=regularizers.L2(0.001),\n",
    "                                  input_shape=(numberOfSamples, 5, 10)\n",
    "                                 )(previousLayer)\n",
    "    \n",
    "outputLayer = layers.Dense(outputDimension, activation='sigmoid')(previousLayer)\n",
    "\n",
    "#compile our model\n",
    "ourModel = Model(inputs=inputLayer, outputs=[outputLayer], name='longitude_cnn')\n",
    "ourModel.compile(loss='mean_squared_error', metrics=[tf.keras.metrics.RootMeanSquaredError()], optimizer='adam')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696074cb-d5a8-42b3-a46e-e95a096d9e3e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#train the mlp\n",
    "trainingDataFlat = trainingData.reshape((-1, inputDimension))\n",
    "ourModel.fit(trainingDataFlat, longitudeLabels, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a22f904-94a4-4fe4-80dc-a449e158153c",
   "metadata": {},
   "source": [
    "### CNN (new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bec33b3-f4c6-4b51-a58a-f04eb51bbcbb",
   "metadata": {},
   "source": [
    "#### Split into training/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1d6181-1677-40be-bf36-03b787cb2407",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedLabels = np.concatenate((np.reshape(longitudeLabels, (-1,1)), np.reshape(latitudeLabels, (-1,1))), axis=1)\n",
    "trainInputs, valInputs, trainLabels, valLabels = train_test_split(dataset[\"samples\"], combinedLabels, test_size=0.5, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb0eaf3-bc03-4919-ae92-a0180062d3dc",
   "metadata": {},
   "source": [
    "#### Construct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe62585c-d137-4f66-98e8-64eadd1bca07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify input dimensionality\n",
    "inputDimension = samples[0].shape[0]*samples[0].shape[1]\n",
    "outputDimension = 2\n",
    "\n",
    "#construct our neural network\n",
    "hiddenLayerSizes = [32, 64, 128, 256]\n",
    "\n",
    "#set up our input layer\n",
    "inputLayer = layers.Input(shape=(sequenceLength, nFeatures))\n",
    "\n",
    "#set up our hidden layers\n",
    "curLayer = 0\n",
    "previousLayer = inputLayer\n",
    "for i,curLayerSize in enumerate(hiddenLayerSizes):\n",
    "    previousLayer = layers.Conv1D(curLayerSize,\n",
    "                                  kernel_size=3,\n",
    "                                 activation='relu', \n",
    "                                  padding=\"same\",\n",
    "                                 name=str(curLayerSize)+'_hiddenLayer'+str(i),\n",
    "                                  kernel_regularizer=regularizers.L2(0.001)\n",
    "                                )(previousLayer)\n",
    "#     previousLayer = layers.LeakyReLU(alpha=0.3)(previousLayer)\n",
    "    previousLayer = layers.Dropout(\n",
    "                                    rate=0.5\n",
    "                                    )(previousLayer)\n",
    "previousLayer = layers.Flatten()(previousLayer)\n",
    "    \n",
    "outputLayer = layers.Dense(outputDimension, activation='linear')(previousLayer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcce964-77f4-4514-95af-57c2b9425d92",
   "metadata": {},
   "source": [
    "#### Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c399c87-6fba-46db-bb98-1537fd33b4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile separate models\n",
    "combinedCNN = Model(inputs=inputLayer, outputs=[outputLayer], name='combined_mlp')\n",
    "combinedCNN.compile(loss='mean_squared_error', metrics=[tf.keras.metrics.RootMeanSquaredError()], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3ab3e9-4c9c-4457-b6ef-7e46f7e7c1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedCNN.save(\"models/cnn.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d8d86c-1b8a-446e-8bbc-c0765d8b136c",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11cdd89-7f44-47e2-893c-3d22cd1b9124",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#train the mlp\n",
    "combinedHistory = combinedCNN.fit(trainInputs, trainLabels, epochs=25, batch_size=32, callbacks=[PlotLossesKeras()], validation_data=(valInputs, valLabels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfeccdf-f2b1-48fa-aa7c-95c1c69fc89d",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bc9160-14c4-4eec-ae5a-e16a644a70d0",
   "metadata": {},
   "source": [
    "##### Predict Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d13d8d-c58c-45ca-9042-91ef38c14bc4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = combinedModel.predict(valInputs)\n",
    "\n",
    "scatterData = pd.DataFrame({\n",
    "    \"index\":np.arange(predictions[:,0].shape[0]),\n",
    "    \"long_predicted\": predictions[:,0].reshape(predictions[:,0].shape[0]),\n",
    "    \"long_actual\": valLabels[:,0].reshape(valLabels[:,0].shape[0]),\n",
    "    \"lat_predicted\": predictions[:,1].reshape(predictions[:,1].shape[0]),\n",
    "    \"lat_actual\": valLabels[:,1].reshape(valLabels[:,1].shape[0])\n",
    "})\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=scatterData.index, y=scatterData.long_predicted, name=\"long_predicted\"))\n",
    "fig.add_trace(go.Scatter(x=scatterData.index, y=scatterData.long_actual, name=\"long_actual\"))\n",
    "fig.add_trace(go.Scatter(x=scatterData.index, y=scatterData.lat_predicted, name=\"lat_predicted\"))\n",
    "fig.add_trace(go.Scatter(x=scatterData.index, y=scatterData.lat_actual, name=\"lat_actual\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daadf233-16ac-4899-9ddb-47e884bb4a1a",
   "metadata": {},
   "source": [
    "##### Predict Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dec6b0d-f1ba-4ae8-9b52-3b2150bd6594",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = combinedModel.predict(trainInputs)\n",
    "\n",
    "scatterData = pd.DataFrame({\n",
    "    \"index\":np.arange(predictions[:,0].shape[0]),\n",
    "    \"long_predicted\": predictions[:,0].reshape(predictions[:,0].shape[0]),\n",
    "    \"long_actual\": trainLabels[:,0].reshape(trainLabels[:,0].shape[0]),\n",
    "    \"lat_predicted\": predictions[:,1].reshape(predictions[:,1].shape[0]),\n",
    "    \"lat_actual\": trainLabels[:,1].reshape(trainLabels[:,1].shape[0])\n",
    "})\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=scatterData.index, y=scatterData.long_predicted, name=\"long_predicted\"))\n",
    "fig.add_trace(go.Scatter(x=scatterData.index, y=scatterData.long_actual, name=\"long_actual\"))\n",
    "fig.add_trace(go.Scatter(x=scatterData.index, y=scatterData.lat_predicted, name=\"lat_predicted\"))\n",
    "fig.add_trace(go.Scatter(x=scatterData.index, y=scatterData.lat_actual, name=\"lat_actual\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea4c17c-e3c5-4a37-9550-6ebfeacb6216",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdfdb47-1d3e-4bfa-9233-ad9bdb453dbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputDimension = 1\n",
    "\n",
    "#construct our neural network\n",
    "hiddenLayerSizes = [512]\n",
    "\n",
    "#set up our input layer\n",
    "\n",
    "#Will work for 2D convolution\n",
    "inputLayer = layers.Input(shape=(sequenceLength, nFeatures))\n",
    "# inputLayer = layers.Input(shape=(5, 10))\n",
    "\n",
    "#set up our hidden layers\n",
    "curLayer = 0\n",
    "previousPreviousLayer = None\n",
    "previousLayer = inputLayer\n",
    "for i,curLayerSize in enumerate(hiddenLayerSizes):\n",
    "    previousPreviousLayer = previousLayer\n",
    "    previousLayer = layers.Conv1D(curLayerSize, 3,\n",
    "                                  activation='relu',\n",
    "                                  name='hiddenLayer_'+str(i),\n",
    "                                  kernel_regularizer=regularizers.L2(0.001),\n",
    "                                  padding=\"same\"\n",
    "                                 )(tf.keras.layers.Concatenate(axis=1)([previousPreviousLayer, previousLayer]))\n",
    "previousLayer = layers.Flatten()(previousLayer)\n",
    "outputLayer = layers.Dense(outputDimension, activation='linear')(previousLayer)\n",
    "\n",
    "#compile our model\n",
    "# opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "longResnet = Model(inputs=inputLayer, outputs=[outputLayer], name='longitude_resnet')\n",
    "latResnet = Model(inputs=inputLayer, outputs=[outputLayer], name='latitude_resnet')\n",
    "longResnet.compile(loss='mean_squared_error', metrics=[tf.keras.metrics.RootMeanSquaredError()], optimizer=\"adam\")\n",
    "latResnet.compile(loss='mean_squared_error', metrics=[tf.keras.metrics.RootMeanSquaredError()], optimizer=\"adam\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb17289-c5c1-4e8f-8dbc-533694687972",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Training longitude model:\")\n",
    "longModelHistory = longResnet.fit(trainInputs, trainLabels[:,0], epochs=25, batch_size=32, callbacks=[PlotLossesKeras()], validation_data=(valInputs, valLabels[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16911014-527f-4b2a-bb2c-b3004f061678",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Training latitude model:\")\n",
    "latModelHistory = latResnet.fit(trainInputs, trainLabels[:,1], epochs=25, batch_size=32, callbacks=[PlotLossesKeras()], validation_data=(valInputs, valLabels[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b43619-e8b8-4d30-a9f3-a1389d45cbe5",
   "metadata": {},
   "source": [
    "#### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7413c73c-9ce1-47b6-b7be-7011ab2b0926",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "longPredictions = longModel.predict(valInputs)\n",
    "latPredictions = latModel.predict(valInputs)\n",
    "\n",
    "scatterData = pd.DataFrame({\n",
    "    \"index\":np.arange(longPredictions.shape[0]),\n",
    "    \"long_predicted\": longPredictions.reshape(longPredictions.shape[0]),\n",
    "    \"long_actual\": valLabels[:,0].reshape(valLabels[:,0].shape[0]),\n",
    "    \"lat_predicted\": latPredictions.reshape(latPredictions.shape[0]),\n",
    "    \"lat_actual\": valLabels[:,1].reshape(valLabels[:,1].shape[0])\n",
    "})\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=scatterData.index, y=scatterData.long_predicted, name=\"long_predicted\"))\n",
    "fig.add_trace(go.Scatter(x=scatterData.index, y=scatterData.long_actual, name=\"long_actual\"))\n",
    "fig.add_trace(go.Scatter(x=scatterData.index, y=scatterData.lat_predicted, name=\"lat_predicted\"))\n",
    "fig.add_trace(go.Scatter(x=scatterData.index, y=scatterData.lat_actual, name=\"lat_actual\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff418bbe-1585-4dca-aca7-df4de3f63247",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "latPredictions.all() == longPredictions.all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e49d2c7-cf52-43f9-a224-d787cc973ebc",
   "metadata": {},
   "source": [
    "### Combined long-lat resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f493e6-e1b6-4ac8-b275-f4e18ea6ea9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputDimension = 2\n",
    "\n",
    "#construct our neural network\n",
    "hiddenLayerSizes = [50, 32, 32, 16, 16, 8]\n",
    "\n",
    "inputLayer = layers.Input(shape=(sequenceLength, nFeatures))\n",
    "\n",
    "#set up our hidden layers\n",
    "# curLayer = 0\n",
    "# previousPreviousLayer = None\n",
    "# previousLayer = inputLayer\n",
    "# for i,curLayerSize in enumerate(hiddenLayerSizes):\n",
    "#     previousPreviousLayer = previousLayer\n",
    "#     previousLayer = layers.Conv1D(curLayerSize, 3,\n",
    "#                                   activation='relu',\n",
    "#                                   name='hiddenLayer_'+str(i),\n",
    "#                                   kernel_regularizer=regularizers.L2(0.001),\n",
    "#                                   padding=\"same\"\n",
    "#                                  )(tf.keras.layers.Concatenate(axis=1)([previousPreviousLayer, previousLayer]))\n",
    "# previousLayer = layers.Flatten()(previousLayer)\n",
    "\n",
    "def convolutionalLayer(size, name):\n",
    "    return layers.Conv1D(curLayerSize, 3,\n",
    "                                  activation='relu',\n",
    "                                  name=name,\n",
    "                                  kernel_regularizer=regularizers.L2(0.001),\n",
    "                                  padding=\"same\")\n",
    "\n",
    "modelLayers = [inputLayer]\n",
    "\n",
    "modelLayers.append(convolutionalLayer(50, \"conv0\")(modelLayers[-1]))\n",
    "\n",
    "# modelLayers.append(layers.Concatenate(axis=1)([modelLayers[-1], modelLayers[-2]]))\n",
    "modelLayers.append(convolutionalLayer(32, \"conv1\")(modelLayers[-1]))\n",
    "\n",
    "modelLayers.append(layers.Concatenate(axis=1)([modelLayers[-1], modelLayers[-2]]))\n",
    "modelLayers.append(convolutionalLayer(32, \"conv2\")(modelLayers[-1]))\n",
    "\n",
    "modelLayers.append(layers.Concatenate(axis=1)([modelLayers[-1], modelLayers[-3]]))\n",
    "modelLayers.append(convolutionalLayer(16, \"conv3\")(modelLayers[-1]))\n",
    "\n",
    "modelLayers.append(layers.Concatenate(axis=1)([modelLayers[-1], modelLayers[-3]]))\n",
    "modelLayers.append(convolutionalLayer(16, \"conv4\")(modelLayers[-1]))\n",
    "\n",
    "modelLayers.append(layers.Concatenate(axis=1)([modelLayers[-1], modelLayers[-3]]))\n",
    "modelLayers.append(convolutionalLayer(8, \"conv5\")(modelLayers[-1]))\n",
    "\n",
    "modelLayers.append(layers.Flatten(name=\"flatten1\")(modelLayers[-1]))\n",
    "\n",
    "\n",
    "outputLayer = layers.Dense(outputDimension, activation='linear')(modelLayers[-1])\n",
    "\n",
    "#compile our model\n",
    "combinedResnet = Model(inputs=inputLayer, outputs=[outputLayer], name='combined_resnet')\n",
    "combinedResnet.compile(loss='mean_squared_error', metrics=[tf.keras.metrics.RootMeanSquaredError()], optimizer=\"adam\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcca4935-7868-49a2-ac2a-d6d51ce7a8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedResnet.save(\"models/resnet.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab9d1e9-6870-4ead-8fc2-69976f2af4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training combined model:\")\n",
    "combinedModelHistory = combinedResnet.fit(trainInputs, trainLabels, epochs=25, batch_size=32, callbacks=[PlotLossesKeras()], validation_data=(valInputs, valLabels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3f1062-5842-4a04-9bae-79c6bae87639",
   "metadata": {},
   "source": [
    "### Squeeze Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438cc05a-dc91-4ec1-90f1-63a0e8fcea84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clipping_relu(x, alpha=1.1):\n",
    "    # pass through relu\n",
    "    # y = K.relu(y, max_value=1)\n",
    "    return tf.clip_by_value(tf.nn.elu(x),\n",
    "                                    tf.constant(-1.0),\n",
    "                                    tf.constant(alpha))\n",
    "\n",
    "#construct our neural network\n",
    "convActivation = \"relu\"\n",
    "kernelSize = 5\n",
    "batchSize = 32\n",
    "dropoutRate = 0.3\n",
    "\n",
    "def convolutionalLayer(size, name):\n",
    "    return layers.Conv1D(size, kernel_size=kernelSize, activation=convActivation, name=name, activity_regularizer=regularizers.l2(1e-2), padding=\"same\")\n",
    "\n",
    "inputLayer = layers.Input(shape=(sequenceLength, trainInputs.shape[2]), name=\"input\")\n",
    "modelLayers = [inputLayer]\n",
    "\n",
    "def layerByName(n):\n",
    "    for layer in modelLayers:\n",
    "        if layer.name.split(\"/\")[0] == n:\n",
    "            return layer\n",
    "    return None\n",
    "\n",
    "modelLayers.append(convolutionalLayer(16, \"conv0\")(modelLayers[-1]))\n",
    "modelLayers.append(layers.Dropout(rate=dropoutRate, name=\"drop0\")(modelLayers[-1]))\n",
    "\n",
    "modelLayers.append(convolutionalLayer(8, \"conv1\")(modelLayers[-1]))\n",
    "modelLayers.append(layers.Dropout(rate=dropoutRate, name=\"drop1\")(modelLayers[-1]))\n",
    "\n",
    "modelLayers.append(convolutionalLayer(4, \"conv2\")(modelLayers[-1]))\n",
    "modelLayers.append(layers.Dropout(rate=dropoutRate, name=\"drop2\")(modelLayers[-1]))\n",
    "\n",
    "modelLayers.append(convolutionalLayer(2, \"conv3\")(modelLayers[-1]))\n",
    "modelLayers.append(layers.Dropout(rate=dropoutRate, name=\"drop3\")(modelLayers[-1]))\n",
    "\n",
    "modelLayers.append(convolutionalLayer(2, \"conv4\")(modelLayers[-1]))\n",
    "modelLayers.append(layers.Dropout(rate=dropoutRate, name=\"drop4\")(modelLayers[-1]))\n",
    "\n",
    "modelLayers.append(layers.Concatenate(axis=1)([modelLayers[-1], layerByName(\"drop3\")]))\n",
    "modelLayers.append(convolutionalLayer(4, \"conv5\")(modelLayers[-1]))\n",
    "modelLayers.append(layers.Dropout(rate=dropoutRate, name=\"drop5\")(modelLayers[-1]))\\\n",
    "\n",
    "modelLayers.append(layers.Concatenate(axis=1)([modelLayers[-1], layerByName(\"drop2\")]))\n",
    "modelLayers.append(convolutionalLayer(8, \"conv6\")(modelLayers[-1]))\n",
    "modelLayers.append(layers.Dropout(rate=dropoutRate, name=\"drop6\")(modelLayers[-1]))\n",
    "\n",
    "modelLayers.append(layers.Concatenate(axis=1)([modelLayers[-1], layerByName(\"drop1\")]))\n",
    "modelLayers.append(convolutionalLayer(16, \"conv7\")(modelLayers[-1]))\n",
    "modelLayers.append(layers.Dropout(rate=dropoutRate, name=\"drop7\")(modelLayers[-1]))\n",
    "\n",
    "modelLayers.append(layers.Concatenate(axis=1)([modelLayers[-1], layerByName(\"drop0\")]))\n",
    "modelLayers.append(layers.Dense(64, activation=convActivation, name=\"dense0\", kernel_regularizer=regularizers.l2(1e-4))(modelLayers[-1]))\n",
    "\n",
    "modelLayers.append(layers.Flatten(name=\"flatten1\")(modelLayers[-1]))\n",
    "outputLayer = layers.Dense(2, activation='linear', name=\"output\")(modelLayers[-1])\n",
    "\n",
    "combinedAdvResnet = Model(inputs=inputLayer, outputs=[outputLayer], name='combined_resnet')\n",
    "combinedAdvResnet.compile(loss='mean_squared_error', metrics=[tf.keras.metrics.RootMeanSquaredError()], optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e356694-5ce7-4068-b4a7-4d45abdc2012",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training combined model:\")\n",
    "combinedModelHistory = combinedAdvResnet.fit(trainInputs, trainLabels, epochs=25, batch_size=32, callbacks=[PlotLossesKeras()], validation_data=(valInputs, valLabels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0958ea8-3ae7-42c1-acb4-8bd72c26a7c3",
   "metadata": {},
   "source": [
    "#### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2808340-46ed-48c3-a309-c4e65d8f5028",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = combinedModel.predict(valInputs)\n",
    "scatterData = pd.DataFrame({\n",
    "    \"index\":np.arange(predictions[:,0].shape[0]),\n",
    "    \"long_predicted\": predictions[:,0].reshape(predictions[:,0].shape[0]),\n",
    "    \"long_actual\": valLabels[:,0].reshape(valLabels[:,0].shape[0]),\n",
    "    \"lat_predicted\": predictions[:,1].reshape(predictions[:,1].shape[0]),\n",
    "    \"lat_actual\": valLabels[:,1].reshape(valLabels[:,1].shape[0])\n",
    "})\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=scatterData.index, y=scatterData.long_predicted, name=\"long_predicted\"))\n",
    "fig.add_trace(go.Scatter(x=scatterData.index, y=scatterData.long_actual, name=\"long_actual\"))\n",
    "fig.add_trace(go.Scatter(x=scatterData.index, y=scatterData.lat_predicted, name=\"lat_predicted\"))\n",
    "fig.add_trace(go.Scatter(x=scatterData.index, y=scatterData.lat_actual, name=\"lat_actual\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9211cd2b-251c-4131-81e2-d0322b7d3cc8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = combinedModel.predict(trainInputs)\n",
    "scatterData = pd.DataFrame({\n",
    "    \"index\":np.arange(predictions[:,0].shape[0]),\n",
    "    \"long_predicted\": predictions[:,0].reshape(predictions[:,0].shape[0]),\n",
    "    \"long_actual\": trainLabels[:,0].reshape(trainLabels[:,0].shape[0]),\n",
    "    \"lat_predicted\": predictions[:,1].reshape(predictions[:,1].shape[0]),\n",
    "    \"lat_actual\": trainLabels[:,1].reshape(trainLabels[:,1].shape[0])\n",
    "})\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=scatterData.index, y=scatterData.long_predicted, name=\"long_predicted\"))\n",
    "fig.add_trace(go.Scatter(x=scatterData.index, y=scatterData.long_actual, name=\"long_actual\"))\n",
    "fig.add_trace(go.Scatter(x=scatterData.index, y=scatterData.lat_predicted, name=\"lat_predicted\"))\n",
    "fig.add_trace(go.Scatter(x=scatterData.index, y=scatterData.lat_actual, name=\"lat_actual\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cddc44-de52-4dcf-8e92-ae56deb92693",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Combined Resnet with Absolute labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085b7856-742d-409a-bd5a-b9241bec2ff4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "longitudeLabels = []\n",
    "latitudeLabels = []\n",
    "\n",
    "for i,window in enumerate(stackedData[:-1]):\n",
    "    last = dataByLocation.iloc[i]\n",
    "    lastLong = last.Longitude\n",
    "    lastLat = last.Latitude\n",
    "    cur = dataByLocation.iloc[i+5]\n",
    "    curLong = cur.Longitude\n",
    "    curLat = cur.Latitude\n",
    "    \n",
    "    longitudeLabels.append(curLong)\n",
    "    latitudeLabels.append(curLat)\n",
    "\n",
    "longitudeLabels = np.array(longitudeLabels)\n",
    "latitudeLabels = np.array(latitudeLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e5d039-d409-4593-929c-c37582488217",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=longitudeLabels, y=latitudeLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061fb9db-35a3-492b-98ce-4358c481ec81",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sequences x samples x features\n",
    "\n",
    "#specify input dimensionality\n",
    "# numberOfSamples = trainingData.shape[0]\n",
    "inputDimension = 50\n",
    "outputDimension = 2\n",
    "\n",
    "#construct our neural network\n",
    "hiddenLayerSizes = [50, 32, 32, 16, 16, 8]\n",
    "\n",
    "#set up our input layer\n",
    "\n",
    "#Will work for 2D convolution\n",
    "inputLayer = layers.Input(shape=(5, 10, 1))\n",
    "# inputLayer = layers.Input(shape=(5, 10))\n",
    "\n",
    "#set up our hidden layers\n",
    "curLayer = 0\n",
    "previousPreviousLayer = None\n",
    "previousLayer = inputLayer\n",
    "for i,curLayerSize in enumerate(hiddenLayerSizes):\n",
    "    previousPreviousLayer = previousLayer\n",
    "    previousLayer = layers.Conv2D(curLayerSize, (3,3),\n",
    "#                                   activation='relu',\n",
    "                                  name='hiddenLayer_'+str(i),\n",
    "                                  kernel_regularizer=regularizers.L2(0.001),\n",
    "                                  padding=\"same\"\n",
    "                                 )(tf.keras.layers.Concatenate(axis=1)([previousPreviousLayer, previousLayer]))\n",
    "previousLayer = layers.Flatten()(previousLayer)\n",
    "outputLayer = layers.Dense(outputDimension, activation='linear')(previousLayer)\n",
    "\n",
    "#compile our model\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "combinedModel = Model(inputs=inputLayer, outputs=[outputLayer], name='absolute_combined_resnet')\n",
    "combinedModel.compile(loss='mean_squared_error', metrics=[tf.keras.metrics.RootMeanSquaredError()], optimizer=opt)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239c5482-e2ca-4b54-a598-86085bbf0b16",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "longTrainInputs, longValInputs, longTrainLabels, longValLabels = train_test_split(trainingData, longitudeLabels, test_size=0.5, shuffle=False)\n",
    "latTrainInputs, latValInputs, latTrainLabels, latValLabels = train_test_split(trainingData, latitudeLabels, test_size=0.5, shuffle=False)\n",
    "\n",
    "combinedTrainLabels = np.array(tuple(zip(longTrainLabels, latTrainLabels)))\n",
    "combinedValLabels = np.array(tuple(zip(longValLabels, latValLabels)))\n",
    "\n",
    "# tensorboard = TensorBoard(log_dir='logs/{}'.format(time()))\n",
    "print(\"Training combined model:\")\n",
    "combinedModelHistory = combinedModel.fit(longTrainInputs, combinedTrainLabels, epochs=10, validation_data=(longValInputs, combinedValLabels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2f2eed-60d6-4d5d-91d8-50dfe136c6fd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "#### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c187ac24-0a0c-487e-be98-bd9dbe941d03",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "combinedPredictions = combinedModel.predict(longValInputs)\n",
    "\n",
    "longPredictions = combinedPredictions[:,0]\n",
    "latPredictions = combinedPredictions[:,1]\n",
    "\n",
    "scatterData = pd.DataFrame({\n",
    "    \"index\":np.arange(longPredictions.shape[0]),\n",
    "    \"long_predicted\": longPredictions,\n",
    "    \"long_actual\": longValLabels.reshape(longValLabels.shape[0]),\n",
    "    \"lat_predicted\": latPredictions,\n",
    "    \"lat_actual\": latValLabels.reshape(latValLabels.shape[0])\n",
    "})\n",
    "scatterData.describe()\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig1 = go.Figure()\n",
    "fig1.add_trace(go.Scatter(x=scatterData.index, y=scatterData.long_predicted, name=\"long_predicted\"))\n",
    "fig1.add_trace(go.Scatter(x=scatterData.index, y=scatterData.long_actual, name=\"long_actual\"))\n",
    "fig2 = go.Figure()\n",
    "fig2.add_trace(go.Scatter(x=scatterData.index, y=scatterData.lat_predicted, name=\"lat_predicted\"))\n",
    "fig2.add_trace(go.Scatter(x=scatterData.index, y=scatterData.lat_actual, name=\"lat_actual\"))\n",
    "\n",
    "fig1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2130b93a-6502-4088-a411-c0c6b7c717b9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a009daf9-b53a-4559-86d3-61eba4c5c202",
   "metadata": {},
   "outputs": [],
   "source": [
    "testDriveFile = open('otherLargeFiles/AllDrives/drive-2021-04-08-13-28-56.pkl', 'rb')\n",
    "testDrive = pickle.load(testDriveFile)\n",
    "testDriveFile.close()\n",
    "testDrive[\"samples\"] = testDrive[\"samples\"]\n",
    "\n",
    "testInputs = testDrive[\"samples\"]\n",
    "testLabels = testDrive[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e73874-3a4c-429e-a843-a095eacad59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tracePredictions(predictions):\n",
    "    scatterData = pd.DataFrame({\n",
    "        \"index\":np.arange(predictions[:,0].shape[0]),\n",
    "        \"long_predicted\": predictions[:,0].reshape(predictions[:,0].shape[0]),\n",
    "        \"long_actual\": testLabels[:,0].reshape(testLabels[:,0].shape[0]),\n",
    "        \"lat_predicted\": predictions[:,1].reshape(predictions[:,1].shape[0]),\n",
    "        \"lat_actual\": testLabels[:,1].reshape(testLabels[:,1].shape[0])\n",
    "    })\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=scatterData.index, y=scatterData.long_predicted, name=\"long_predicted\"))\n",
    "    fig.add_trace(go.Scatter(x=scatterData.index, y=scatterData.long_actual, name=\"long_actual\"))\n",
    "    fig.add_trace(go.Scatter(x=scatterData.index, y=scatterData.lat_predicted, name=\"lat_predicted\"))\n",
    "    fig.add_trace(go.Scatter(x=scatterData.index, y=scatterData.lat_actual, name=\"lat_actual\"))\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23e9488-7dac-430d-92df-e677b93e9a14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = np.hstack([longMLP.predict(testInputs),latMLP.predict(testInputs)])\n",
    "tracePredictions(predictions).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0ada77-8b05-49c2-91f1-e175f59096e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = combinedMLP.predict(testInputs)\n",
    "tracePredictions(predictions).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb188e8-d96d-4aa1-8021-fc4aebd6bf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = combinedCNN.predict(testInputs)\n",
    "tracePredictions(predictions).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407455ce-d8ce-406a-abf9-29e10af3e95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.hstack([longResnet.predict(testInputs),latResnet.predict(testInputs)])\n",
    "tracePredictions(predictions).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc948e8-6da9-4fc0-ab66-2072970184b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = combinedResnet.predict(testInputs)\n",
    "tracePredictions(predictions).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea05b720-e33a-43c5-8e6c-4e0831c8a182",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = combinedAdvResnet.predict(testInputs)\n",
    "tracePredictions(predictions).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a136d239-2aac-4b31-8509-0338487a22fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
