{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c0647e7-f352-492d-835b-8fce71921c1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ipywidgets import widgets\n",
    "from sklearn import preprocessing\n",
    "import sklearn\n",
    "from datetime import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import plotly.graph_objects as go\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79fac01a-8cc6-4052-a7c8-1873c8cf65c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nDrives = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "779ddea8-b7cb-4a93-90a8-97e0349c0800",
   "metadata": {},
   "outputs": [],
   "source": [
    "drivePaths = [str(path) for path in Path(\".\").rglob(\"outputs/withNewFeatures/dataByLocation*.csv\")]\n",
    "# drivePaths = [\"./outputs/withNewFeatures/dataByLocation_2021-06-08-15-01-31_2T3Y1RFV8KC014025.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81575c5a-3130-4928-9bb4-b77bc42a91e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 drives that meet specifications\n"
     ]
    }
   ],
   "source": [
    "subsamplingPeriod = 1\n",
    "\n",
    "drivesWithLocation = []\n",
    "drivesWithoutLocation = []\n",
    "driveIDs = []\n",
    "for drivePath in drivePaths:\n",
    "    drive = pd.read_csv(drivePath)\n",
    "    if len(drive) > 1200 and \"dataByLocation_2021-06-03-19-38-35_2T3Y1RFV8KC014025.csv\" not in drivePath:\n",
    "        driveIDs.append(\"_\".join(drivePath.split(\"/\")[-1].split(\"_\")[1:-1]))\n",
    "        drive = drive.iloc[::subsamplingPeriod]\n",
    "        driveWithoutLocation = drive.drop(columns=[\"Time\",\"Longitude\", \"Latitude\"])\n",
    "#         driveWithoutLocation = driveWithoutLocation.drop(columns=[\"ZAcceleration\", \"LongAcceleration\", \"LatAcceleration\"])\n",
    "        drivesWithLocation.append(drive)\n",
    "        drivesWithoutLocation.append(driveWithoutLocation)\n",
    "print(\"Found\", len(drivesWithoutLocation), \"drives that meet specifications\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d650130-45fa-4016-915b-38a5a93df700",
   "metadata": {},
   "source": [
    "## Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9769f4d6-3443-4ffd-ba5f-a6f72af1cada",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerFile = open('otherLargeFiles/scalerUsedForTrainingInputs.pkl', 'rb')\n",
    "scaler = pickle.load(scalerFile)                     \n",
    "scalerFile.close()\n",
    "\n",
    "normalizedDrives = []\n",
    "for drive in drivesWithoutLocation:\n",
    "    drive = drive.values[:]\n",
    "    data_normalized = scaler.transform(drive)\n",
    "    data_normalized = pd.DataFrame(data_normalized)\n",
    "    data_normalized[0] = drive[:,0]\n",
    "    normalizedDrives.append(data_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a1847f-bc0f-4ef9-9d33-63dc759ad71f",
   "metadata": {},
   "source": [
    "## Window (discrete) and Generate Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09088cd4-cece-4363-a3cf-c2695bc1b15a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on drive 0  Has 1800 windows worth of samples\n"
     ]
    }
   ],
   "source": [
    "# Faster\n",
    "sequenceLength = 10\n",
    "\n",
    "windowsPerDrive = []\n",
    "for drive in normalizedDrives:\n",
    "    nWindows = math.floor(len(drive) / sequenceLength)\n",
    "    if len(drive) % sequenceLength == 0:\n",
    "        nWindows-=1\n",
    "    windowsPerDrive.append(nWindows)\n",
    "    \n",
    "datasetLength = sum(windowsPerDrive)\n",
    "dataset = {\"samples\":np.full((datasetLength, sequenceLength, len(normalizedDrives[0].columns)), -1.),\"labels\":np.full((datasetLength, 2), -1.)}\n",
    "datasetIndex = 0\n",
    "\n",
    "for k,drive in enumerate(normalizedDrives):\n",
    "    nWindows = windowsPerDrive[k]\n",
    "    print(\"on drive\",k, \" Has\", nWindows, \"windows worth of samples\")\n",
    "    for i in range(nWindows):\n",
    "        windowStartIndex = i * sequenceLength\n",
    "        windowEndIndex = windowStartIndex + sequenceLength\n",
    "        window = drive.iloc[windowStartIndex:windowEndIndex].to_numpy()\n",
    "        startWithLocation = drivesWithLocation[k].iloc[windowStartIndex]\n",
    "        startLong = startWithLocation.Longitude\n",
    "        startLat = startWithLocation.Latitude\n",
    "        nextWithLocation = drivesWithLocation[k].iloc[windowEndIndex]\n",
    "        nextLong = nextWithLocation.Longitude\n",
    "        nextLat = nextWithLocation.Latitude\n",
    "        deltaLong = nextLong - startLong\n",
    "        deltaLat = nextLat - startLat\n",
    "        deltas = [deltaLong, deltaLat]\n",
    "        dataset[\"samples\"][datasetIndex] = window\n",
    "        dataset[\"labels\"][datasetIndex] = deltas\n",
    "        datasetIndex+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5253e5e4-a20e-4634-bc65-779fde42bed2",
   "metadata": {},
   "source": [
    "### Normalize labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffecab3c-4706-46b2-a9d8-5556bd226aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerFile = open('otherLargeFiles/scalerUsedForTrainingLabels.pkl', 'rb')\n",
    "scaler = pickle.load(scalerFile)                     \n",
    "scalerFile.close()\n",
    "\n",
    "originalLabels = dataset[\"labels\"]\n",
    "labels_normalized = scaler.transform(originalLabels)\n",
    "dataset[\"labels\"] = labels_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d5a041-74af-4253-8542-d80b81637edf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f91e90e-35b4-42aa-a2ba-11df51b7a932",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nextTestNumber = max([int(str(path).split(\".\")[0].split(\"-\")[-1]) for path in Path(\"./otherLargeFiles\").rglob(\"test-drive*\")], default=-1) + 1\n",
    "dbfile = open(f'otherLargeFiles/test-drive-{str(nextTestNumber)}.pkl', 'ab')\n",
    "\n",
    "# source, destination\n",
    "pickle.dump(dataset, dbfile)                     \n",
    "dbfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cbd3b0-5c56-4da6-b13d-feb15c2089e5",
   "metadata": {},
   "source": [
    "MORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c386bcce-2d05-4a5c-af08-763c4ce7770c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTestDrive(path):\n",
    "    drivePaths = [path]\n",
    "    subsamplingPeriod = 1\n",
    "\n",
    "    drivesWithLocation = []\n",
    "    drivesWithoutLocation = []\n",
    "    driveIDs = []\n",
    "    for drivePath in drivePaths:\n",
    "        drive = pd.read_csv(drivePath)\n",
    "        if len(drive) > 1200 and \"dataByLocation_2021-06-03-19-38-35_2T3Y1RFV8KC014025.csv\" not in drivePath:\n",
    "            driveIDs.append(\"_\".join(drivePath.split(\"/\")[-1].split(\"_\")[1:-1]))\n",
    "            drive = drive.iloc[::subsamplingPeriod]\n",
    "            driveWithoutLocation = drive.drop(columns=[\"Time\",\"Longitude\", \"Latitude\"])\n",
    "    #         driveWithoutLocation = driveWithoutLocation.drop(columns=[\"ZAcceleration\", \"LongAcceleration\", \"LatAcceleration\"])\n",
    "            drivesWithLocation.append(drive)\n",
    "            drivesWithoutLocation.append(driveWithoutLocation)\n",
    "    print(\"Found\", len(drivesWithoutLocation), \"drives that meet specifications\")\n",
    "    if len(drivesWithoutLocation) > 0:\n",
    "        scalerFile = open('otherLargeFiles/scalerUsedForTrainingInputs.pkl', 'rb')\n",
    "        scaler = pickle.load(scalerFile)                     \n",
    "        scalerFile.close()\n",
    "\n",
    "        normalizedDrives = []\n",
    "        for drive in drivesWithoutLocation:\n",
    "            drive = drive.values[:]\n",
    "            data_normalized = scaler.transform(drive)\n",
    "            data_normalized = pd.DataFrame(data_normalized)\n",
    "            data_normalized[0] = drive[:,0]\n",
    "            normalizedDrives.append(data_normalized)\n",
    "\n",
    "        # Faster\n",
    "        sequenceLength = 10\n",
    "\n",
    "        windowsPerDrive = []\n",
    "        for drive in normalizedDrives:\n",
    "            nWindows = math.floor(len(drive) / sequenceLength)\n",
    "            if len(drive) % sequenceLength == 0:\n",
    "                nWindows-=1\n",
    "            windowsPerDrive.append(nWindows)\n",
    "\n",
    "        datasetLength = sum(windowsPerDrive)\n",
    "        dataset = {\"samples\":np.full((datasetLength, sequenceLength, len(normalizedDrives[0].columns)), -1.),\"labels\":np.full((datasetLength, 2), -1.)}\n",
    "        datasetIndex = 0\n",
    "\n",
    "        for k,drive in enumerate(normalizedDrives):\n",
    "            nWindows = windowsPerDrive[k]\n",
    "            print(\"on drive\",k, \" Has\", nWindows, \"windows worth of samples\")\n",
    "            for i in range(nWindows):\n",
    "                windowStartIndex = i * sequenceLength\n",
    "                windowEndIndex = windowStartIndex + sequenceLength\n",
    "                window = drive.iloc[windowStartIndex:windowEndIndex].to_numpy()\n",
    "                startWithLocation = drivesWithLocation[k].iloc[windowStartIndex]\n",
    "                startLong = startWithLocation.Longitude\n",
    "                startLat = startWithLocation.Latitude\n",
    "                nextWithLocation = drivesWithLocation[k].iloc[windowEndIndex]\n",
    "                nextLong = nextWithLocation.Longitude\n",
    "                nextLat = nextWithLocation.Latitude\n",
    "                deltaLong = nextLong - startLong\n",
    "                deltaLat = nextLat - startLat\n",
    "                deltas = [deltaLong, deltaLat]\n",
    "                dataset[\"samples\"][datasetIndex] = window\n",
    "                dataset[\"labels\"][datasetIndex] = deltas\n",
    "                datasetIndex+=1\n",
    "        scalerFile = open('otherLargeFiles/scalerUsedForTrainingLabels.pkl', 'rb')\n",
    "        scaler = pickle.load(scalerFile)                     \n",
    "        scalerFile.close()\n",
    "\n",
    "        originalLabels = dataset[\"labels\"]\n",
    "        labels_normalized = scaler.transform(originalLabels)\n",
    "        dataset[\"labels\"] = labels_normalized\n",
    "        nextTestNumber = max([int(str(path).split(\".\")[0].split(\"-\")[-1]) for path in Path(\"./otherLargeFiles/AllDrives\").rglob(\"drive*\")], default=-1) + 1\n",
    "        dbfile = open(f'otherLargeFiles/AllDrives/drive-{driveIDs[0]}.pkl', 'ab')\n",
    "\n",
    "        # source, destination\n",
    "        pickle.dump(dataset, dbfile)                     \n",
    "        dbfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "719a46e1-e53d-4c1f-8b2b-a2b4539d8171",
   "metadata": {},
   "outputs": [],
   "source": [
    "drivesNotInDataset = []\n",
    "drivesInDatasetFile = open(f'otherLargeFiles/drives-used.pkl', 'rb')\n",
    "drivesInDataset = pickle.load(drivesInDatasetFile)\n",
    "drivesInDatasetFile.close()\n",
    "for drivePath in drivePaths:\n",
    "    if drivePath not in drivesInDataset:\n",
    "        drivesNotInDataset.append(drivePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27da10e7-6232-41fc-84dd-ad0f00a2c1fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'drivesNotInDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-637bcbca903b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdrivesNotInDataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mcreateTestDrive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# for path in [str(path) for path in Path(\".\").rglob(\"outputs/withNewFeatures/dataByLocation*.csv\")]:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#     createTestDrive(path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'drivesNotInDataset' is not defined"
     ]
    }
   ],
   "source": [
    "for path in drivesNotInDataset:\n",
    "    createTestDrive(path)\n",
    "# for path in [str(path) for path in Path(\".\").rglob(\"outputs/withNewFeatures/dataByLocation*.csv\")]:\n",
    "#     createTestDrive(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0095534-2313-4e0f-ac8e-81ac32bd3119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "drivesInDatasetFile = open(f'otherLargeFiles/drives-used.pkl', 'rb')\n",
    "drivesInDataset = pickle.load(drivesInDatasetFile)\n",
    "drivesInDatasetFile.close()\n",
    "'2021-04-08-13-28-56' in [drivePath.split(\"/\")[-1].split(\".\")[0].split(\"_\")[1] for drivePath in drivesInDataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da764106-1ab0-4fe7-a7fc-7b04a0512adc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
