{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea27a965-cdb5-4c43-9fa2-17c909eeb091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from ipywidgets import widgets\n",
    "from sklearn import preprocessing\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras import layers, regularizers,Model, utils\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from datetime import time\n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import train_test_split\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.model_selection import train_test_split\n",
    "from livelossplot import PlotLossesKeras\n",
    "\n",
    "from os import listdir\n",
    "import os\n",
    "#thanks to https://stackoverflow.com/users/218681/bernhard-kausler\n",
    "def find_csv_filenames( path_to_dir, suffix=\".csv\" ):\n",
    "    filenames = listdir(path_to_dir)\n",
    "    return [ filename for filename in filenames if filename.endswith( suffix ) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7db9e70-237c-44f4-8408-c9ee0f6f2674",
   "metadata": {},
   "outputs": [],
   "source": [
    "drivePaths = find_csv_filenames(os.getcwd())\n",
    "subsamplingPeriod = 32\n",
    "sequenceLength = 16\n",
    "smoothingSigma = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc312bc-0600-41aa-a793-254041edf753",
   "metadata": {},
   "outputs": [],
   "source": [
    "drivesWithLocation = []\n",
    "drivesWithoutLocation = []\n",
    "for drivePath in drivePaths:\n",
    "    drive = pd.read_csv(drivePath)\n",
    "    drive = drive.iloc[::subsamplingPeriod].copy(deep=True)\n",
    "    driveWithoutLocation = drive.drop(columns=[\"Time\", \"Longitude\", \"Latitude\"])\n",
    "    drivesWithLocation.append(drive)\n",
    "    drivesWithoutLocation.append(driveWithoutLocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e013c0-d555-45b5-be15-2cb5fc3c9fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizedDrives = []\n",
    "for drive in drivesWithoutLocation:\n",
    "    standard_scaler = preprocessing.StandardScaler()\n",
    "    data_normalized = standard_scaler.fit_transform(drive)\n",
    "    data_normalized = pd.DataFrame(data_normalized)\n",
    "    normalizedDrives.append(data_normalized.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7de2e48-000b-4a04-abe2-e31315ed9e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "roughData = np.vstack(normalizedDrives[:])\n",
    "print(roughData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4b2094-3f86-4c7d-b7e6-2087da57117a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(drivePaths)):\n",
    "    print(drivePath[k])\n",
    "    roughData = normalizedDrives[k]\n",
    "    smoothData = -1*np.ones(shape=roughData.shape)\n",
    "    features = range(roughData.shape[1])\n",
    "    for curFeature in features:\n",
    "        print(\"smoothing %s\"%curFeature)\n",
    "\n",
    "        smoothData[:,curFeature] = gaussian_filter(roughData[:,curFeature], sigma=smoothingSigma)#.rolling(window=sequenceLength, win_type='gaussian', center=True).mean(std=0.5)\n",
    "    #     ewmaSmoothData[curFeature] = pd.DataFrame.ewm(data_normalized[curFeature], span=sequenceLength)\n",
    "\n",
    "        #plot the original feature and the smoothed feature\n",
    "        scatterData = pd.DataFrame({\n",
    "        \"index\":range(roughData.shape[0]),\n",
    "        \"smoothData\": smoothData[:,curFeature],\n",
    "        #\"ewmaSmoothData\": ewmaSmoothData[curFeature],\n",
    "        \"originalData\": roughData[:,curFeature],\n",
    "        })\n",
    "        scatterData.describe()\n",
    "\n",
    "        fig1 = go.Figure()\n",
    "    #     fig1.add_trace(go.Scatter(x=scatterData.index, y=scatterData.ewmaSmoothData, name=\"ewma smoothed data\"))\n",
    "        fig1.add_trace(go.Scatter(x=scatterData.index, y=scatterData.smoothData, name=\"gaussian smoothed data\"))\n",
    "        fig1.add_trace(go.Scatter(x=scatterData.index, y=scatterData.originalData, name=\"original data\"))\n",
    "\n",
    "        fig1.show()\n",
    "    \n",
    "    \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b89dc6-c739-4100-baad-c56573d3483f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Speed',\n",
    "            'LatAcceleration',\n",
    "            'LongAcceleration',\n",
    "            'SteerTorque',\n",
    "            'SteerRate',\n",
    "            'SteerAngle',\n",
    "            'FLWheelSpeed',\n",
    "            'FRWheelSpeed',\n",
    "            'RRWheelSpeed',\n",
    "            'RLWheelSpeed']\n",
    "windowedDrives = []\n",
    "\n",
    "stackedData = []\n",
    "# split can_data into subsampled sequences\n",
    "for drive in normalizedDrives:\n",
    "    for i in range(smoothData.shape[0]-sequenceLength):\n",
    "        stackedData.append(smoothData[i:i+sequenceLength,:])\n",
    "    stackedData = np.array(stackedData)\n",
    "    windowedDrives.append(stackedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9586e917-0753-424e-8f76-20894507b94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\"samples\":[], \"labels\":[]}\n",
    "for k,drive in enumerate(windowedDrives):\n",
    "    for i,window in enumerate(drive[:(-1-sequenceLength-1)]):\n",
    "        last = drivesWithLocation[k].iloc[i]\n",
    "        lastLong = last.Longitude\n",
    "        lastLat = last.Latitude\n",
    "        cur = drivesWithLocation[k].iloc[i+sequenceLength]\n",
    "        curLong = cur.Longitude\n",
    "        curLat = cur.Latitude\n",
    "        \n",
    "        dataset[\"samples\"].append(window)\n",
    "        dataset[\"labels\"].append([curLong - lastLong, curLat - lastLat])\n",
    "data = np.array(dataset[\"samples\"][:])\n",
    "labels = np.array(dataset[\"labels\"][:])\n",
    "\n",
    "print(data.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4713c55c-c528-4b42-9279-eb25e7f3cd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the labels\n",
    "rawLabels = np.copy(labels)\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "labels = scaler.fit_transform(rawLabels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71ffd67-71cd-4898-bcfd-f8f2808bc4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tfMI\n",
    "\n",
    "#parameters\n",
    "numberOfEDASamples = 100\n",
    "\n",
    "edaSubset, edaLatLabels, edaLongLabels = data[:numberOfEDASamples, :, :], labels[:numberOfEDASamples,1], labels[:numberOfEDASamples, 0]\n",
    "\n",
    "#estimate mutual information between windows and lattitude/longitudes\n",
    "flattenedEDAData = edaSubset.reshape((edaSubset.shape[0], edaSubset.shape[1]*edaSubset.shape[2]))\n",
    "print(flattenedEDAData.shape)\n",
    "print(edaLatLabels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa72e9e-fdab-474d-882d-bc2b8db9c956",
   "metadata": {},
   "outputs": [],
   "source": [
    "latInfo = tfMI.callMIGPU(flattenedEDAData, edaLatLabels.reshape(-1,1), \n",
    "                           alpha=1.01)\n",
    "longInfo = tfMI.callMIGPU(flattenedEDAData, edaLongLabels.reshape(-1,1), \n",
    "                           alpha=1.01)\n",
    "\n",
    "print(latInfo)\n",
    "print(longInfo)\n",
    "\n",
    "#iterate over features and calculate each feature's MI with labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9743afe-efb0-4dcb-8dba-8f0d13715011",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data\n",
    "longTrainInputs, longTestInputs, longTrainLabels, longTestLabels = train_test_split(data, labels[:,0], test_size=0.5, shuffle=False)\n",
    "latTrainInputs, latValInputs, latTrainLabels, latValLabels = train_test_split(data, labels[:,1], test_size=0.5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed97858-1b5f-4d4f-ba10-f07c0e6833bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(longTrainInputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884395ec-d22c-4605-8d11-19163e662dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define our activation\n",
    "def clipping_relu(x, alpha=1.1):\n",
    "            # pass through relu\n",
    "            # y = K.relu(y, max_value=1)\n",
    "            return tf.clip_by_value(tf.nn.elu(x),\n",
    "                                            tf.constant(-1.0),\n",
    "                                            tf.constant(alpha))\n",
    "\n",
    "#specify input dimensionality\n",
    "numberOfSamples = longTrainInputs.shape[0]\n",
    "numberOfChannels = longTrainInputs.shape[2]\n",
    "outputDimension = 1\n",
    "dropoutRate = 0.33\n",
    "#construct our neural network\n",
    "hiddenLayerSizes = [32, 64, 128]\n",
    "\n",
    "#set up our input layer\n",
    "inputLayer = layers.Input(shape=(sequenceLength,numberOfChannels))\n",
    "\n",
    "#set up our hidden layers\n",
    "curLayer = 0\n",
    "previousLayer = inputLayer\n",
    "for curLayerSize in hiddenLayerSizes:\n",
    "    previousLayer = layers.Conv1D(curLayerSize, int(np.ceil(sequenceLength/3)),\n",
    "                                  activation=clipping_relu,\n",
    "                                  padding='same',\n",
    "                                  kernel_regularizer=regularizers.l2(0.0001),\n",
    "#                                   activity_regularizer=regularizers.l2(0.001),\n",
    "                                  name=str(curLayer)\n",
    "                                 )(previousLayer)\n",
    "#     previousLayer = layers.BatchNormalization()(previousLayer)\n",
    "# \n",
    "    curLayer+=1\n",
    "\n",
    "# previousLayer = layers.Dropout(0.5)(previousLayer)\n",
    "# previousLayer = layers.MaxPool1D(2)(previousLayer)\n",
    "    \n",
    "previousLayer = layers.Dense(2048, kernel_regularizer=regularizers.l2(0.0001),\n",
    "                             activation=clipping_relu)(previousLayer)\n",
    "outputLayer = layers.Dense(outputDimension, activation='sigmoid')(previousLayer)\n",
    "\n",
    "#compile our model\n",
    "ourModel = Model(inputs=inputLayer, outputs=[outputLayer], name='longitude_cnn')\n",
    "ourModel.compile(loss='mean_squared_error', metrics=[tf.keras.metrics.RootMeanSquaredError()], optimizer='adam')\n",
    "ourModel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fe74c6-de96-45a5-bc59-6b3d13e0e553",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa31b0d-2757-471e-b89e-1cafb103a310",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingEpochs = 64\n",
    "#fit our model on the long data\n",
    "ourModel.fit(longTrainInputs, longTrainLabels, epochs=trainingEpochs, validation_split=0.15, \n",
    "             callbacks=[PlotLossesKeras()], shuffle=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50473651-826d-4505-9d71-5319e2895f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the neural network\n",
    "testPreds = ourModel.predict(longTestInputs)\n",
    "print(testPreds.shape)\n",
    "# testPreds = testPreds.squeeze()\n",
    "predictedLongs = np.mean(testPreds, axis=1).reshape(-1,)\n",
    "\n",
    "print(predictedLongs.shape)\n",
    "print(longTestLabels.shape)\n",
    "\n",
    "scatterData = pd.DataFrame({\n",
    "\"index\":range(predictedLongs.shape[0]),\n",
    "\"longPreds\": predictedLongs,\n",
    "\"longTruth\": longTestLabels,\n",
    "})\n",
    "scatterData.describe()\n",
    "\n",
    "fig1 = go.Figure()\n",
    "#     fig1.add_trace(go.Scatter(x=scatterData.index, y=scatterData.ewmaSmoothData, name=\"ewma smoothed data\"))\n",
    "fig1.add_trace(go.Scatter(x=scatterData.index, y=scatterData.longPreds, name=\"predicted longitude\"))\n",
    "fig1.add_trace(go.Scatter(x=scatterData.index, y=scatterData.longTruth, name=\"true longitude\"))\n",
    "\n",
    "fig1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da221259-8fa3-45e0-88ff-c2aea37b01d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd36c50b-871c-4369-b1c2-2fcbfbcb8b06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e62176b-8b16-4d78-bc78-1e85fb728d1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
